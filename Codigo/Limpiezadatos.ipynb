{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Script de depuracion completa de series horarias ambientales\n",
    "Diseñado para su ejecucion local. Comentarios y cadenas en español.\n",
    "Dependencias: pandas, numpy, scikit-learn, joblib, xgboost (opcional)\n",
    "Ejecutar: python depuracion_pipeline.py --input ruta_fichero.csv [--stations_meta ruta_meta.csv]\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import IsolationForest, RandomForestRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb  # noqa\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "# Carpeta de salida\n",
    "OUT_DIR = \"depuracion_output\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Variables esperadas en los datos\n",
    "VARIABLES = [\n",
    "    \"O3\", \"NO\", \"NO2\", \"NOx\",\n",
    "    \"wind_speed\", \"wind_dir\", \"temp\", \"pressure\", \"rad\"\n",
    "]\n",
    "\n",
    "# Limites de deteccion orientativos\n",
    "LOD = {\n",
    "    \"O3\": 1.0,\n",
    "    \"NO\": 0.1,\n",
    "    \"NO2\": 0.1,\n",
    "    \"NOx\": 0.1,\n",
    "    \"wind_speed\": 0.01,\n",
    "    \"wind_dir\": 0.0,\n",
    "    \"temp\": -273.15,\n",
    "    \"pressure\": 300.0,\n",
    "    \"rad\": 0.0\n",
    "}\n",
    "\n",
    "# Rangos plausibles orientativos\n",
    "RANGOS_PLAUSIBLES = {\n",
    "    \"O3\": (0.0, 1000.0),\n",
    "    \"NO\": (0.0, 500.0),\n",
    "    \"NO2\": (0.0, 500.0),\n",
    "    \"NOx\": (0.0, 1000.0),\n",
    "    \"wind_speed\": (0.0, 80.0),\n",
    "    \"wind_dir\": (0.0, 360.0),\n",
    "    \"temp\": (-50.0, 60.0),\n",
    "    \"pressure\": (250.0, 1100.0),\n",
    "    \"rad\": (0.0, 2000.0)\n",
    "}\n",
    "\n",
    "# Parametros detectores e imputacion\n",
    "HAMPEL_WINDOW_H = 12\n",
    "HAMPEL_NSIGMA = 3.5\n",
    "IFOREST_CONTAMINATION = 0.02\n",
    "KNN_K = 5\n",
    "ITERATIVE_IMPUTATIONS = 5\n",
    "NOX_TOLERANCE = 20.0\n",
    "\n",
    "# -----------------------\n",
    "# Funciones utilitarias\n",
    "# -----------------------\n",
    "\n",
    "def ensure_datetime_index(df, time_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Asegura indice datetime con zona Europe Madrid y resample horario.\n",
    "    Mantiene columnas no numericas rellenando por propagacion.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    if time_col in df2.columns:\n",
    "        df2[time_col] = pd.to_datetime(df2[time_col], errors=\"coerce\")\n",
    "        df2 = df2.set_index(time_col)\n",
    "    else:\n",
    "        if not isinstance(df2.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"Se requiere columna timestamp o indice datetime\")\n",
    "    # normalizar zona horaria\n",
    "    try:\n",
    "        if df2.index.tz is None:\n",
    "            df2.index = df2.index.tz_localize(\"Europe/Madrid\")\n",
    "        else:\n",
    "            df2.index = df2.index.tz_convert(\"Europe/Madrid\")\n",
    "    except Exception:\n",
    "        df2.index = df2.index.tz_localize(None)\n",
    "        df2.index = df2.index.tz_localize(\"Europe/Madrid\")\n",
    "    # separar columnas numericas y no numericas\n",
    "    numeric_cols = df2.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    non_numeric_cols = [c for c in df2.columns if c not in numeric_cols]\n",
    "    # resample numericas por media\n",
    "    if len(numeric_cols) > 0:\n",
    "        df_num = df2[numeric_cols].resample(\"H\").mean()\n",
    "    else:\n",
    "        df_num = pd.DataFrame(index=pd.date_range(start=df2.index.min(), end=df2.index.max(), freq=\"H\", tz=\"Europe/Madrid\"))\n",
    "    # resample no numericas por ffill\n",
    "    df_non = pd.DataFrame(index=df_num.index)\n",
    "    for c in non_numeric_cols:\n",
    "        ser = df2[c].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "        ser_r = ser.reindex(df_num.index, method=\"ffill\")\n",
    "        df_non[c] = ser_r\n",
    "    df_out = pd.concat([df_num, df_non], axis=1)\n",
    "    return df_out\n",
    "\n",
    "def mark_non_detects(df, variables=VARIABLES, lod=LOD):\n",
    "    for v in variables:\n",
    "        flag = f\"{v}_nd\"\n",
    "        if v in df.columns and v in lod:\n",
    "            df[flag] = df[v].notna() & (df[v] < lod[v])\n",
    "        else:\n",
    "            df[flag] = False\n",
    "    return df\n",
    "\n",
    "def plausibility_flags(df, variables=VARIABLES, ranges=RANGOS_PLAUSIBLES):\n",
    "    for v in variables:\n",
    "        col = f\"{v}_plausible\"\n",
    "        if v in df.columns and v in ranges:\n",
    "            low, high = ranges[v]\n",
    "            df[col] = df[v].between(low, high) | df[v].isna()\n",
    "        else:\n",
    "            df[col] = True\n",
    "    return df\n",
    "\n",
    "def hampel_filter(series, window_H=HAMPEL_WINDOW_H, n_sigma=HAMPEL_NSIGMA):\n",
    "    k = int(window_H)\n",
    "    if k < 1:\n",
    "        k = 1\n",
    "    rolled_med = series.rolling(window=2*k+1, center=True, min_periods=1).median()\n",
    "    mad = series.rolling(window=2*k+1, center=True, min_periods=1).apply(lambda x: np.median(np.abs(x - np.median(x))), raw=True)\n",
    "    threshold = n_sigma * 1.4826 * mad\n",
    "    diff = (series - rolled_med).abs()\n",
    "    outliers = diff > threshold\n",
    "    return outliers.fillna(False)\n",
    "\n",
    "def iqr_outliers(series, window_days=7, factor=1.5):\n",
    "    win = int(window_days * 24)\n",
    "    if win < 1:\n",
    "        win = 24\n",
    "    flags = pd.Series(False, index=series.index)\n",
    "    for start in range(0, len(series), win):\n",
    "        block = series.iloc[start:start+win]\n",
    "        if len(block) == 0:\n",
    "            continue\n",
    "        q1 = np.nanpercentile(block, 25)\n",
    "        q3 = np.nanpercentile(block, 75)\n",
    "        iqr = q3 - q1\n",
    "        low = q1 - factor * iqr\n",
    "        high = q3 + factor * iqr\n",
    "        fl = (block < low) | (block > high)\n",
    "        flags.iloc[start:start+win] = fl.values\n",
    "    return flags.fillna(False)\n",
    "\n",
    "def robust_zscore(series, window_H=24, th=3.0):\n",
    "    win = int(window_H)\n",
    "    med = series.rolling(window=win, min_periods=1, center=True).median()\n",
    "    mad = series.rolling(window=win, min_periods=1, center=True).apply(lambda x: np.median(np.abs(x - np.median(x))), raw=True)\n",
    "    denom = mad.replace(0, np.nan) * 1.4826\n",
    "    z = (series - med) / denom\n",
    "    return (z.abs() > th).fillna(False)\n",
    "\n",
    "def isolation_forest_flags(df_vars, contamination=IFOREST_CONTAMINATION, random_state=0):\n",
    "    df_med = df_vars.fillna(df_vars.median())\n",
    "    iso = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "    iso.fit(df_med.values)\n",
    "    preds = iso.predict(df_med.values)\n",
    "    flags = pd.Series(preds == -1, index=df_vars.index)\n",
    "    return flags\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000.0\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "# -----------------------\n",
    "# Deteccion interestacional\n",
    "# -----------------------\n",
    "\n",
    "def interstation_anomaly(df, station_col, var, stations_meta, radius_km=50):\n",
    "    \"\"\"\n",
    "    df debe tener columna station y timestamp en el indice\n",
    "    stations_meta DataFrame con columnas station lat lon\n",
    "    \"\"\"\n",
    "    df_reset = df.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "    pivot = df_reset.pivot(index=\"timestamp\", columns=station_col, values=var)\n",
    "    neighbors = {}\n",
    "    for st in stations_meta[\"station\"].unique():\n",
    "        lat = stations_meta.loc[stations_meta[\"station\"] == st, \"lat\"].values[0]\n",
    "        lon = stations_meta.loc[stations_meta[\"station\"] == st, \"lon\"].values[0]\n",
    "        nbrs = []\n",
    "        for st2 in stations_meta[\"station\"].unique():\n",
    "            if st2 == st:\n",
    "                continue\n",
    "            lat2 = stations_meta.loc[stations_meta[\"station\"] == st2, \"lat\"].values[0]\n",
    "            lon2 = stations_meta.loc[stations_meta[\"station\"] == st2, \"lon\"].values[0]\n",
    "            d = haversine(lat, lon, lat2, lon2) / 1000.0\n",
    "            if d <= radius_km:\n",
    "                nbrs.append(st2)\n",
    "        neighbors[st] = nbrs\n",
    "\n",
    "    flags = pd.Series(False, index=pivot.index)\n",
    "    for st in pivot.columns:\n",
    "        nbrs = neighbors.get(st, [])\n",
    "        if len(nbrs) == 0:\n",
    "            continue\n",
    "        median_neighbors = pivot[nbrs].median(axis=1)\n",
    "        diff = (pivot[st] - median_neighbors).abs()\n",
    "        neigh_std = pivot[nbrs].std(axis=1).replace(0, np.nan)\n",
    "        flag_series = diff > 3 * neigh_std\n",
    "        flags |= flag_series.fillna(False)\n",
    "    return flags\n",
    "\n",
    "# -----------------------\n",
    "# Imputacion\n",
    "# -----------------------\n",
    "\n",
    "def classify_gaps(series):\n",
    "    mask = series.isna()\n",
    "    gaps = []\n",
    "    if mask.sum() == 0:\n",
    "        return gaps\n",
    "    in_gap = False\n",
    "    start = None\n",
    "    prev_t = None\n",
    "    for t, m in mask.iteritems():\n",
    "        if m and not in_gap:\n",
    "            in_gap = True\n",
    "            start = t\n",
    "        elif not m and in_gap:\n",
    "            end = prev_t\n",
    "            length = int((end - start) / np.timedelta64(1, \"h\")) + 1\n",
    "            gaps.append((start, end, length))\n",
    "            in_gap = False\n",
    "        prev_t = t\n",
    "    if in_gap:\n",
    "        end = prev_t\n",
    "        length = int((end - start) / np.timedelta64(1, \"h\")) + 1\n",
    "        gaps.append((start, end, length))\n",
    "    return gaps\n",
    "\n",
    "def impute_short(series):\n",
    "    return series.interpolate(method=\"time\", limit_direction=\"both\")\n",
    "\n",
    "def impute_medium(block_df, variables=VARIABLES, k=KNN_K):\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    arr = imputer.fit_transform(block_df[variables])\n",
    "    block_imp = block_df.copy()\n",
    "    block_imp[variables] = arr\n",
    "    return block_imp, imputer\n",
    "\n",
    "def impute_long_model(df_block, target_var, features, model_type=\"rf\", random_state=0):\n",
    "    df_feat = df_block[features + [target_var]].copy()\n",
    "    train = df_feat[df_feat[target_var].notna()].dropna()\n",
    "    predict_idx = df_feat[df_feat[target_var].isna()].index\n",
    "    if len(train) < 50:\n",
    "        return df_block, None\n",
    "    X = train[features].values\n",
    "    y = train[target_var].values\n",
    "    if model_type == \"xgb\" and XGBOOST_AVAILABLE:\n",
    "        dtrain = xgb.DMatrix(X, label=y)\n",
    "        params = {\"objective\": \"reg:squarederror\", \"verbosity\": 0}\n",
    "        bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "        if len(predict_idx) > 0:\n",
    "            Xpred = df_feat.loc[predict_idx, features].fillna(df_feat[features].median()).values\n",
    "            ypred = bst.predict(xgb.DMatrix(Xpred))\n",
    "            df_block.loc[predict_idx, target_var] = ypred\n",
    "        return df_block, bst\n",
    "    else:\n",
    "        rf = RandomForestRegressor(n_estimators=200, random_state=random_state, n_jobs=None)\n",
    "        rf.fit(X, y)\n",
    "        if len(predict_idx) > 0:\n",
    "            Xpred = df_feat.loc[predict_idx, features].fillna(df_feat[features].median()).values\n",
    "            ypred = rf.predict(Xpred)\n",
    "            df_block.loc[predict_idx, target_var] = ypred\n",
    "        return df_block, rf\n",
    "\n",
    "def multiple_imputation_iterative(df_block, variables=VARIABLES, M=ITERATIVE_IMPUTATIONS, random_state=0):\n",
    "    imputations = []\n",
    "    for m in range(M):\n",
    "        imp = IterativeImputer(sample_posterior=True, random_state=random_state + m, max_iter=10)\n",
    "        arr = imp.fit_transform(df_block[variables])\n",
    "        df_imp = df_block.copy()\n",
    "        df_imp[variables] = arr\n",
    "        imputations.append(df_imp)\n",
    "    return imputations\n",
    "\n",
    "# -----------------------\n",
    "# Consistencia y reportes\n",
    "# -----------------------\n",
    "\n",
    "def check_nox_consistency(df, tolerance=NOX_TOLERANCE):\n",
    "    if not all(v in df.columns for v in [\"NO\", \"NO2\", \"NOx\"]):\n",
    "        df[\"nox_consistent\"] = np.nan\n",
    "        return df\n",
    "    diff = np.abs(df[\"NOx\"] - (df[\"NO\"].fillna(0) + df[\"NO2\"].fillna(0)))\n",
    "    df[\"nox_consistent\"] = diff <= tolerance\n",
    "    df[\"nox_diff\"] = diff\n",
    "    return df\n",
    "\n",
    "def quality_report(df, variables=VARIABLES):\n",
    "    report = {}\n",
    "    total = len(df)\n",
    "    for v in variables:\n",
    "        n_missing = int(df[v].isna().sum()) if v in df.columns else 0\n",
    "        n_nd = int(df.get(f\"{v}_nd\", pd.Series(False, index=df.index)).sum())\n",
    "        n_implausible = int((~df.get(f\"{v}_plausible\", pd.Series(True, index=df.index))).sum())\n",
    "        report[v] = {\n",
    "            \"total\": int(total),\n",
    "            \"missing\": n_missing,\n",
    "            \"missing_frac\": float(n_missing / total) if total > 0 else 0.0,\n",
    "            \"nd_count\": n_nd,\n",
    "            \"implausible_count\": n_implausible\n",
    "        }\n",
    "    out_path = os.path.join(OUT_DIR, \"quality_report.json\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "    return report\n",
    "\n",
    "# -----------------------\n",
    "# Pipeline principal\n",
    "# -----------------------\n",
    "\n",
    "def depuration_pipeline(df, stations_meta=None, station_col=\"station\"):\n",
    "    meta = {}\n",
    "    # sincronizar y resample horario\n",
    "    if \"timestamp\" not in df.columns and not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"El dataframe debe contener columna timestamp o indice datetime\")\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df = df.copy()\n",
    "    df = df.reset_index().rename(columns={\"index\": \"timestamp\"}) if isinstance(df.index, pd.DatetimeIndex) and \"timestamp\" not in df.columns else df\n",
    "    df = ensure_datetime_index(df, time_col=\"timestamp\")\n",
    "\n",
    "    # marcar no detectables y plausibilidad\n",
    "    df = mark_non_detects(df)\n",
    "    df = plausibility_flags(df)\n",
    "\n",
    "    # deteccion de anomalias univariantes\n",
    "    anomaly_flags = pd.DataFrame(index=df.index)\n",
    "    for v in VARIABLES:\n",
    "        if v not in df.columns:\n",
    "            continue\n",
    "        s = df[v]\n",
    "        ham = hampel_filter(s)\n",
    "        iqr = iqr_outliers(s)\n",
    "        rz = robust_zscore(s)\n",
    "        flags = ham | iqr | rz\n",
    "        anomaly_flags[f\"{v}_anomaly\"] = flags\n",
    "\n",
    "    # deteccion multivariante\n",
    "    block = df[VARIABLES].copy()\n",
    "    iso_flags = isolation_forest_flags(block)\n",
    "    anomaly_flags[\"multivar_anomaly\"] = iso_flags\n",
    "\n",
    "    # comparacion interestacional si hay metadatos\n",
    "    if stations_meta is not None and \"station\" in df.columns:\n",
    "        for v in VARIABLES:\n",
    "            try:\n",
    "                interest_flags = interstation_anomaly(df, station_col, v, stations_meta)\n",
    "                anomaly_flags[f\"{v}_interstation\"] = interest_flags.reindex(df.index, fill_value=False)\n",
    "            except Exception:\n",
    "                anomaly_flags[f\"{v}_interstation\"] = False\n",
    "\n",
    "    # volcar flags en df\n",
    "    for col in anomaly_flags.columns:\n",
    "        df[col] = anomaly_flags[col]\n",
    "\n",
    "    # imputacion jerarquizada por estacion\n",
    "    df_imputed = df.copy()\n",
    "    provenance = {}\n",
    "    stations = df_imputed[\"station\"].unique() if \"station\" in df_imputed.columns else [None]\n",
    "    for st in stations:\n",
    "        if st is None:\n",
    "            sub_idx = df_imputed.index\n",
    "        else:\n",
    "            sub_idx = df_imputed[df_imputed[\"station\"] == st].index\n",
    "        sub = df_imputed.loc[sub_idx].copy()\n",
    "        for v in VARIABLES:\n",
    "            if v not in sub.columns:\n",
    "                continue\n",
    "            gaps = classify_gaps(sub[v])\n",
    "            provenance.setdefault(v, []).append({\"station\": st, \"n_gaps\": len(gaps), \"gaps\": [(str(a), str(b), int(c)) for a, b, c in gaps]})\n",
    "            for start, end, length in gaps:\n",
    "                if length <= 6:\n",
    "                    sub.loc[start:end, v] = impute_short(sub[v].loc[start:end])\n",
    "                    for t in pd.date_range(start, end, freq=\"H\"):\n",
    "                        sub.loc[t, f\"{v}_imputed_method\"] = \"interpolacion_corta\"\n",
    "                elif length <= 72:\n",
    "                    window_start = max(sub.index.min(), start - pd.Timedelta(hours=72))\n",
    "                    window_end = min(sub.index.max(), end + pd.Timedelta(hours=72))\n",
    "                    block = sub.loc[window_start:window_end]\n",
    "                    try:\n",
    "                        block_imp, knn = impute_medium(block, variables=VARIABLES, k=KNN_K)\n",
    "                        sub.loc[window_start:window_end, VARIABLES] = block_imp[VARIABLES]\n",
    "                        for t in pd.date_range(start, end, freq=\"H\"):\n",
    "                            sub.loc[t, f\"{v}_imputed_method\"] = \"knn_multivariante\"\n",
    "                    except Exception:\n",
    "                        sub.loc[start:end, v] = impute_short(sub[v].loc[start:end])\n",
    "                        for t in pd.date_range(start, end, freq=\"H\"):\n",
    "                            sub.loc[t, f\"{v}_imputed_method\"] = \"interpolacion_fallback\"\n",
    "                else:\n",
    "                    features = [c for c in VARIABLES if c != v]\n",
    "                    try:\n",
    "                        sub_filled, model = impute_long_model(sub, v, features, model_type=\"xgb\" if XGBOOST_AVAILABLE else \"rf\")\n",
    "                        sub = sub_filled\n",
    "                        method_name = \"model_xgb\" if (model is not None and XGBOOST_AVAILABLE) else (\"model_rf\" if model is not None else \"imputacion_fallback\")\n",
    "                        for t in pd.date_range(start, end, freq=\"H\"):\n",
    "                            sub.loc[t, f\"{v}_imputed_method\"] = method_name\n",
    "                    except Exception:\n",
    "                        sub.loc[start:end, v] = impute_short(sub[v].loc[start:end])\n",
    "                        for t in pd.date_range(start, end, freq=\"H\"):\n",
    "                            sub.loc[t, f\"{v}_imputed_method\"] = \"interpolacion_fallback\"\n",
    "        df_imputed.loc[sub_idx] = sub\n",
    "\n",
    "    # imputacion multiple para estimar incertidumbre si procede\n",
    "    imputed_fraction = {v: float(df_imputed[v].isna().mean()) if v in df_imputed.columns else 0.0 for v in VARIABLES}\n",
    "    multiple_results = {}\n",
    "    for v in VARIABLES:\n",
    "        if imputed_fraction.get(v, 0.0) > 0.05:\n",
    "            try:\n",
    "                imputations = multiple_imputation_iterative(df_imputed, variables=VARIABLES)\n",
    "                multiple_results[v] = imputations\n",
    "            except Exception:\n",
    "                multiple_results[v] = None\n",
    "\n",
    "    # comprobaciones finales de consistencia\n",
    "    df_final = check_nox_consistency(df_imputed)\n",
    "\n",
    "    # informe de calidad\n",
    "    report = quality_report(df_final)\n",
    "\n",
    "    meta[\"provenance\"] = provenance\n",
    "    meta[\"imputed_fraction\"] = imputed_fraction\n",
    "    meta[\"report\"] = report\n",
    "    meta_path = os.path.join(OUT_DIR, \"depuration_meta.json\")\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    out_csv = os.path.join(OUT_DIR, \"df_depured.csv\")\n",
    "    df_final.reset_index().rename(columns={\"index\": \"timestamp\"}).to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    return df_final, meta, multiple_results\n",
    "\n",
    "# -----------------------\n",
    "# Interfaz linea de comandos\n",
    "# -----------------------\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Pipeline de depuracion de series ambientales\")\n",
    "    parser.add_argument(\"--input\", required=True, help=\"Ruta al CSV de entrada con columna timestamp\")\n",
    "    parser.add_argument(\"--stations_meta\", required=False, help=\"CSV con metadatos de estaciones columnas station lat lon\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    df = pd.read_csv(args.input)\n",
    "    stations_meta = None\n",
    "    if args.stations_meta:\n",
    "        stations_meta = pd.read_csv(args.stations_meta)\n",
    "\n",
    "    df_out, meta, multiple = depuration_pipeline(df, stations_meta=stations_meta)\n",
    "    print(\"Depuracion completada. Ficheros guardados en\", OUT_DIR)\n",
    "    print(\"Resumen calidad por variable:\")\n",
    "    for k, v in meta[\"report\"].items():\n",
    "        print(k, \" missing_frac:\", round(v[\"missing_frac\"], 3), \" missing:\", v[\"missing\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
